{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Parallel Computing \n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture, we'll discuss parallel computing. We'll first lok at the reasons for parallel computing and then looks at some data-science specific parallel applications such as MapReduce and Spark. \n",
    "\n",
    "**Further reading:**\n",
    "\n",
    "[J. Lin and C. Dyer, Data-Intensive Text Processing with MapReduce (2010)](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is parallelism?\n",
    "\n",
    "Traditional algorithms use serial computation, which means that instructions are processed one after the others. These algorithms are easy to implement.\n",
    "\n",
    "**Parallelism** is the approach of executing multiple threads simultaneously. This can be applied to anything. For example, it is common that GUI applications have a thread for the user interface and a separate thread for any back-end processing (saving a file, running a computation), which ensures that the user interface stays responsive while the back-end processing is active. \n",
    "\n",
    "**Parallel computing** is when a computational task is broken into smaller subtasks, which are processed simultaneously and indpendently. \n",
    "\n",
    "So why do we need parallelism? Speed! The benefits of parallel computing are that either larger tasks can be completed that couldn't be completed in reasonable time on a single CPU, or that tasks can be completed faster. \n",
    "\n",
    "## Group Exercise\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "R1: 19 18 7 13 17\n",
    "R2: 19 18 18 11 7 \n",
    "R3: 9 1 5 10 13\n",
    "R4: 20 7 17 17 6 \n",
    "R5: 14 18 5 13 14\n",
    "R6: 19 2 5 11 1  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rise of Parallelism\n",
    "\n",
    "Parellism has always been important, but it is becoming increasingly relevant. Two decades ago, we could rely on increasing CPU speeds to make computation faster and faster. [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law) states that the number of transistors in a chip doubles approximately every two years. And more transistors (naively) equals more computing power.\n",
    "\n",
    "![Moore's Law](moores_law.png)\n",
    "\n",
    "Up to about 2004 this also correlated with an increase in speed for a single CPU core by increasing it's clock speed (number of instructions computed per second). However, since then it hasn't been possible to further increase clock speed for various technical reasons. \n",
    "\n",
    "![Clock Speed](clock_speed.png)\n",
    "\n",
    "Now, instead of making individual CPUs faster, we are adding more CPUs on a single Chip. However, having multiple processors doesn't make a single thread faster. Hence, we increasingly need parallelism to leverage the speed advantages. \n",
    "\n",
    "On a larger scale, we can also distribute workload amonst multiple computers, e.g., in a computer clusters. Also, GPUs (Graphics Processing Units) are now used for a wide variety of tasks, because of their ability to run many processes in parallel. \n",
    "\n",
    "**A Programmers Perspective.** Question: How do you make your program run faster? \n",
    "\n",
    "Answer before 2004: Wait 6 months and buy a new computer. \n",
    "\n",
    "Answer after 2004: You need to write parallel software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:** To speed up the addition of an array of numbers, we can leverage parallelism:\n",
    "\n",
    "![GPGPU Example](gpgpu.png)\n",
    "*Source: Kohei KaiGai*\n",
    "\n",
    "\n",
    "**Example 2:** To use cross validation to search for optimal parameters, you could have put each parameter tested on a different computer. See the `n_jobs` parameter in the scikit-learn command [*GridSearchCV*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "### Parallel Thinking\n",
    "\n",
    " * Decompose work into pieces that can safely be performed in parallel\n",
    " * Assign work to processors\n",
    " * Managing communication/synchronization between the processors so that it does not limit speedup. \n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In a group, develop conceptual strategies to parallelize these operations for the following tasks. Think about how to split the input and how to merge the data. \n",
    "\n",
    " * Multiply all elements of a large array by 4.\n",
    " * Calcualte the mean of a very large array.\n",
    " * For a tabular dataset, group by a categoical dimension and find the minimum of each group.\n",
    "\n",
    "Which of these operations is better to parallelize? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurdles\n",
    "\n",
    "On the other hand, parallel computing is not easy. There are a large number of low-level programming aspects that must be handled. For example, one must consider  \n",
    "- Partitioning input data\n",
    "- Shared memory (Open Multiprocessing (OpenMP)) or distributed memory (Message Passing Interface (MPI)) architectures\n",
    "- Scheduling execution\n",
    "- Handling failures\n",
    "- Interprocessor communication\n",
    "\n",
    "There are a lot of difficult Computer Science questions here! For example, [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) gives the theoretical speedup of a process depending on how much of the task can be parallelized. \n",
    "\n",
    "![Amdahl's Law](amdahls_law.png)\n",
    "\n",
    "*Source: Daniels220, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=6678551*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parallelism in Python\n",
    "\n",
    "Python has support for low-level parallelism; see the \n",
    "[python documentation](https://docs.python.org/3.6/library/concurrency.html), \n",
    "[multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html),\n",
    "[ipyparallel](https://ipyparallel.readthedocs.io/en/latest/index.html), \n",
    "[joblib](https://pypi.python.org/pypi/joblib), \n",
    "*etc*..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll use the multiproceesing library for an example. It uses lower level process spawning API and provides true parellelism by spawning sub processes. It uses the multiple processor cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a naive function to evaluate whether a function is prime or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def isprime(n):\n",
    "    prime = True\n",
    "    for i in arange(2, n):\n",
    "        if n/i % 1 == 0:\n",
    "            return False            \n",
    "    return prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will call the function isprime for integers 1 to max_number. This will be carried out serially one number at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_serial_test(max_number):\n",
    "    [isprime(i) for i in range(max_number)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a pool of processes and will call `isprime` on integers from `1` to `max_number`. Multiple numbers will be processed at the same time, depending on the number of processes and the number of CPUs in your computer. \n",
    "\n",
    "The number of processes spawned in the pool must be less than or equal to number of cores in CPU for this to make sense. Using more processes than CPUs reduces the performance rather than improving it.\n",
    "\n",
    "The critical function here is `pool.map`, which takes a function (`isprime`), an iterable data structure (the array returned by range), and a chunksize (optional). It takes the process of the pool and then gives it chunks of data that are then passed to the `isprime` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_parallel_test(max_number):\n",
    "    # create multiple sub-processes\n",
    "    pool = multiprocessing.Pool(processes = 4)\n",
    "    # pass a chunk of numbers into each process as it becomes available\n",
    "    # the chunksize defines how many are put in in batch\n",
    "    pool.map(isprime, range(max_number), chunksize=100)\n",
    "    # pool.close() will terminate all the subprocesses once their allocated work is done.\n",
    "    pool.close()\n",
    "    # pool.join() makes the main processes wait for subprocesses to complete\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 51.5 ms, total: 12 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%time prime_serial_test(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 21.3 ms, total: 49.5 ms\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%time prime_parallel_test(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results on a machine with 4 physical cores (8 - logical cores due to hyperthreading).\n",
    "\n",
    "| Condition                         | Time     |\n",
    "| :---------------------------------|  --------|\n",
    "| Serial                            | 11.9 sec |\n",
    "| Parallel  (with 2 processes)      | 6.07 sec |\n",
    "| Parallel  (with 4 processes)      | 3.49 sec | \n",
    "| Parallel  (with 8 processes)      | 3.61 sec |\n",
    "| Parallel  (with 16 processes)     | 3.64 sec |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the sum of all pairwise products of an array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5233250317622361, 0.14321663192869138, 0.9554937128974341, 0.48605940530912706, 0.0885312335113213, 0.3022338711843523, 0.9331123487264593, 0.06107177632849614, 0.56063664635997, 0.879535690243721]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "big_array = [random.random() for _ in range(5000)]\n",
    "print (big_array[:10])\n",
    "\n",
    "# This function multiplies each element of vector X with element y\n",
    "# and returns sum of the products.\n",
    "def sum_products(X, y):\n",
    "    return sum(x*y for x in X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 15.5 ms, total: 1.39 s\n",
      "Wall time: 1.38 s\n",
      "6336802.429491523\n"
     ]
    }
   ],
   "source": [
    "# This function calls sum_products for each element of vector X with X itself.\n",
    "def pairwise_sum(X):\n",
    "    return sum(sum_products(X,y) for y in X)\n",
    "\n",
    "# Serial call for pairwise sum.\n",
    "%time results = pairwise_sum(big_array)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.76 ms, sys: 4.71 ms, total: 13.5 ms\n",
      "Wall time: 365 ms\n",
      "The first 10 results of individual workers: [1317.3664287916924, 360.51931686155626, 2405.264919306379, 1223.557644083063, 222.85972932462417, 760.8135124002831, 2348.924297322143, 153.73602063504748, 1411.2909794854609, 2214.055741514053]\n",
      "The Sum 6336802.429491523\n"
     ]
    }
   ],
   "source": [
    "# This function wraps the sum_products function.\n",
    "# pool.map can only take a function and one argument for the function.\n",
    "# To pass two arguments we turn them into a single tuple.\n",
    "# We pass it to sum_products using *args which is variable arguments in python,\n",
    "# this automatically unfolds the tuple into the two arguments needed by sum_products\n",
    "def wrap_sum_products(args):\n",
    "    return sum_products(*args)\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "# Notice how we pass the list to parallel_pairwise_sum with each element of \n",
    "# the list being a tuple as discussed in above comment.\n",
    "%time sub_results = pool.map(wrap_sum_products, ((big_array, y) for y in big_array))\n",
    "print(\"The first 10 results of individual workers:\", sub_results[0:10])\n",
    "# This is a serial step\n",
    "results = sum(sub_results)\n",
    "print(\"The Sum\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "MapReduce is a programming model for  distributed computations. It's designed for processinging large data sets (think ~ 1 terabyte of data) and is essentially a parallel and distributed algorithm. You can easily deploy MapReduce programs on clusters, it will take care of a lot of problems associated with clusters, such as individual nodes dying, etc.  \n",
    "\n",
    "**History:**\n",
    "\n",
    "1. Developed by Google, but built on previously-developed ideas\n",
    "+  Apache Hadoop is an open source implemenation \n",
    "+ implemented in Java\n",
    "+ There are several Python interfaces to Hadoop, including MrJob, etc.... Unfortunately, these don't yet work nicely with Jupyter notebook.\n",
    "\n",
    "We'll be using [MrJob](https://pythonhosted.org/mrjob/) and have taken and modified examples from there. \n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Data is mapped into key-value pairs. The key is a unique identifier of the object, and then the value can store anything.\n",
    "\n",
    "1. Mapping: \n",
    "Typically, the initial format of the key-value pair is very rough. The data is just stored, but\n",
    "not processed much yet, and in any particular order. It may contain many irrelevant parts of information (for\n",
    "the current task). So the mapping phase is in charge of getting it into the right format. Keep in mind, this\n",
    "data set is very large, so it is stored once, but may be used for many different purposes.\n",
    "The mapping phase takes a file, and converts it into another set of key-value pairs. The values generally\n",
    "contain the data of current interest, and the keys are the identifiers we care about.\n",
    "\n",
    "2. Shuffling: \n",
    "The output of the maping step is typically (roughly) as large as the original data, so it also cannot all fit on\n",
    "one machine. The shuffle step puts all key-value pairs with the same key on one machine (if they can all\n",
    "fit). \n",
    "\n",
    "3. Reducing: \n",
    "The reduce step aggregates the data by keys using some useful (application specific) function. This data is now all in the same location – we have locality.\n",
    "\n",
    "![](map_reduce.png)\n",
    "\n",
    "\n",
    "-See Lecture 14 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k) and the accompanying video [here](http://cs109.github.io/2015/pages/videos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's looks at a practical example. Note the use of the [`yield`](https://docs.python.org/3/reference/expressions.html#yieldexpr ) keyword. \n",
    "\n",
    "You'll have to install MRJob: \n",
    "\n",
    "```\n",
    "pip install mrjob \n",
    "```\n",
    "\n",
    "We want to count how many words, characters, and lines are in a text that is stored in a file. Note that his will not execute in the notebook, you'll have to run it on the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    # Takes one line from a file passed as a command line parameter\n",
    "    # and yields three key-value pairs for chars, words and lines. \n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    # Reduces the values associated with each key by summing them. \n",
    "    # Creates sums for chars, words and lines.\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is available in the file [word_frequency.py](word_frequency.py). Here's the (sanitized) output if we run this on Moby Dick, which is stored in `pg2701.txt`. \n",
    "\n",
    "```\n",
    "$python word_frequency.py pg2701.txt\n",
    "\"words\"\t215135\n",
    "\"lines\"\t22108\n",
    "\"chars\"\t1213077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's count the number of specific words in a text.** Here is our model for splitting, mapping, shuffling, and reducing: \n",
    "\n",
    "![](mapreducewc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of this idea, which you can also find in [count_words.py](count_words.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRCountWords(MRJob):\n",
    "\n",
    "    # The mapper creates a key for every word and a value of 1\n",
    "    # Input is a line of text. We ignore keys _\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            # regular expression to remove all non-word characters\n",
    "            word = re.sub(\"[^a-zA-Z]+\", \"\", word)\n",
    "            # make it lowercase and yield the word as key and 1 as value\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    # The reducer sums over all the values of the keys\n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCountWords.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this on the text of \"The Cat in the Hat\" by Dr. Suess:\n",
    "\n",
    "```\n",
    "python count_words.py cat_in_the_hat.txt \n",
    "```\n",
    "And get this output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"seuss\"\t2\n",
    "\"seven\"\t2\n",
    "\"sew\"\t2\n",
    "\"sews\"\t9\n",
    "\"shake\"\t2\n",
    "\"shame\"\t4\n",
    "\"she\"\t7\n",
    "\"sheep\"\t2\n",
    "\"shine\"\t1\n",
    "\"ship\"\t4\n",
    "\"shocking\"\t1\n",
    "\"shoe\"\t2\n",
    "\"shoes\"\t2\n",
    "\"shook\"\t4\n",
    "\"should\"\t15\n",
    "\"shove\"\t1\n",
    "\"show\"\t4\n",
    "\"shut\"\t2\n",
    "\"sick\"\t2\n",
    "\"side\"\t2\n",
    "\"silly\"\t1\n",
    "\"simply\"\t2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Ngram viewer](https://books.google.com/ngrams) is an example where map reduce word freuqencies becomes useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiners\n",
    "\n",
    "This is fine, but we leave a lot of work for the shuffle and sort step. We can do better, by doing a \"local reduce\", which is known as combiners. Combiners work just like reducers, but are executed locally on a single compute node, whereas reducers are executed globally. \n",
    "\n",
    "\n",
    "![](combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combiner has to have the same method signature as the reducer. Sometimes they can be the same, as in this example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRCountWords(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            word = re.sub(\"[^a-zA-Z]+\", \"\", word)\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    # New: the combiner. It has the same implementation as the reducer\n",
    "    # but is execued on the local node only. \n",
    "    def combiner(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "            \n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCountWords.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code that finds the most frequent word in a text. See [most_common.py](most_common.py). The key here is that we run two MapReduce steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "# we store the regular expression in an object\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    # here we explicitly define the steps to take. We have two steps, the first\n",
    "    # has a mapper, combiner and reducer, the second only a reducer.\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        num_occurences = sum(counts)\n",
    "        # we are not yielding a key, so that all outputs are treated together\n",
    "        yield None, (num_occurences, word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MapReduce on servers\n",
    "\n",
    "We've so far only run MapReduce locally and in a single process. MRJob, however, has support for a variety of different architectures. \n",
    "\n",
    "Here is how you run it locally but in multiple processes and simulating Hadoop features: \n",
    "```\n",
    "python most_common.py -r local pg2701.txt\n",
    "```\n",
    "\n",
    "Here is a command that would run it on Amazon Elastic MapReduce [if properly set up](https://pythonhosted.org/mrjob/guides/emr-quickstart.html)\n",
    "\n",
    "```\n",
    "python my_job.py -r emr s3://my-inputs/input.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "How would you use MapReduce to find anagrams? Sketch a strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark can be thought of as  MapReduce 2.0\n",
    "\n",
    "- In memory as opposed to disk\n",
    "- Data can be cached in memory or disk for future use\n",
    "- 100x faster than Hadoop MapReduce in memory or 10x faster on disk\n",
    "- resilient distributed dataset (RDD)\n",
    "- Python, Java, and Scala interfaces\n",
    "- [apache-spark](http://spark.apache.org/) can be used in python through [findspark](https://github.com/minrk/findspark)\n",
    "- Easier than Hadoop while being functional, runs a general DAG\n",
    "\n",
    "For more, see Lecture 15 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k) \n",
    "and accompanying [notebook](https://github.com/cs109/2015/blob/master/Lectures/15b-Spark.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
